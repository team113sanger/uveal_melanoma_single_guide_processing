---
title: "sgRNA CRISPRko - Uveal melanoma (5621 and 5821)"
author: "Victoria Offord"
date: "12/02/2023"
output:
  html_document:
    theme: united
    highlight: textmate
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load_dependencies, include = FALSE}
suppressPackageStartupMessages(suppressWarnings(library(tidyverse)))
suppressPackageStartupMessages(suppressWarnings(library(DT)))
suppressPackageStartupMessages(suppressWarnings(library(ggsci)))
suppressPackageStartupMessages(suppressWarnings(library(ggpubr)))
suppressPackageStartupMessages(suppressWarnings(library(RColorBrewer)))
suppressPackageStartupMessages(suppressWarnings(library(scales)))
suppressPackageStartupMessages(suppressWarnings(library(jsonlite)))
suppressPackageStartupMessages(suppressWarnings(library(GGally)))

# Set repository path
repo_path <- '/lustre/scratch124/casm/team113/projects/5621_5821_sgrna_crisprko_uveal_melanoma'
```

# {.tabset}

## Background

CRISPRko screens were performed in 10 uveal melanoma cell lines expressing Cas9 (3 replicates) using the Human CRISPR Library (Yusa v1.1). Samples at day 14 of screen were quantified along with a previous sequencing run of the pooled plasmids which were used to generate the library.

This project contains:

* 7 cell lines from SequenceScape study 5621: OMM23, MP41, MP38, OMM1, Mel270, MP46, OMM25
* 3 cell lines from SequenceScape study 5821: Mel285, 92.1, Mel202

Only two Mel285 replicates are present in study 5621 (Mel285_c909R2 and Mel285_c909R3) as one replicate failed. As a result, the Mel285 screen was repeats with three replicates in study 5821 (Mel285_R5, Mel285_R6, Mel285_R7). 

## Files

### Generic

| Name | Location | Description |
| :--- | :---| :--- |
| CRISPRcleanR library | `library_CRISPRcleanR_KY_Library_v1.1.tsv` | CRISPRcleanR formatted library |
| Sample metadata | `5621_5821_selected_metadata.tsv` | Sample identifier/name mapping and sequencing statistics (subset of samples in the same Sanger study which are specific to this project) |
| Existing plasmid counts | `C6596666_yusa1.1.tsv` | Plasmid counts generated by methods allowing fuzzy matching (for comparison to pyCROQUET) |

### pyCROQUET 

| Name | Type | Location | Description |
| :--- | :--- | :---| :--- |
| Library | input | `library_CRISPRcleanR_KY_Library_v1.1.pycroquet.tsv` | Guide library re-formatted for [pyCROQUET](https://github.com/cancerit/pycroquet) |
| Sequencing data | input | `DATA/CRAM/study_sample/[study]_[sample].cram` | Sample CRAM (individual lanes combined with `samtools merge` ) and their indices |
| Raw sample counts | output | `DATA/pycroquet/[study]_[sample].counts.tsv.gz ` | Raw sample counts. See [here](https://github.com/cancerit/pycroquet/wiki/Count-file) for format |
| Read-to-guide mapping | output | `DATA/pycroquet/3081_3081STDY6354070.cram` | Top hits from read-to-guide mapping (ignore for single guide analyses) | 
| Mapping statistics | output | `DATA/pycroquet/3081_3081STDY6354070.stats.json` | JSON-formatted statistics relating to sample counts. See [here](https://github.com/cancerit/pycroquet/wiki/Statistics-file) for more detail |

### QC and count matrices

| Name | Location | Description |
| :--- | :---| :--- |
| Raw count matrix | `DATA/01_raw_count_matrix.tsv` | Raw count matrix of 101090 guides with Mel285_c909R2 and Mel285_c909R3 removed and HumanV1_1_rep1 renamed to Plasmid |
| Read mapping barplot | `DATA/QC/read_to_guide_mapping_barplot.png` | Barplot showing percentage of mapped and unmapped reads per sample |
| Low and zero count guide barplot | `DATA/QC/low_count_guides_barplot.png` | Barplot showing percentage of low (< 15 reads) count and zero count guides per sample |
| Plasmid correlation | `DATA/QC/plasmid_correlation.png` | Correlation of existing C659666 plasmid counts (allowing mismatches) and pyCROQUET plasmid counts (exact matching) |
| Mel285 correlation | `DATA/QC/Mel285_correlation.png` | Correlation of Mel285 replicates from two screens (2 replicates and 3 replicates) |

## Metadata

Sequencing metadata ([METADATA/5621_5821_selected_metadata.tsv](../METADATA/5621_5821_selected_metadata.tsv)) was manually collated to select only samples which are of interest to this project.

```{r read_lane_metadata}
# Read in lane-level metadata
lane_metadata <- read.delim(file.path(repo_path, 'METADATA', '5621_5821_selected_metadata.tsv'), header = T, sep = "\t", colClasses = c(rep("character", 10), 'numeric', rep("character", 3)))
```

```{r show_lane_metadata, echo = FALSE}
# Present to user as a searchable table
datatable(lane_metadata, class = 'white-space: nowrap', filter = 'top', options = list(scrollX = TRUE, autoWidth = FALSE, pageLength = 10), rownames = FALSE)
```

Lane CRAMs we combined into per-sample CRAMs using [samtools](https://github.com/samtools/samtools) version 1.14 (`samtools merge`).

```{r sample_metadata}
# Sum read counts and summarise by sample
sample_metadata <- lane_metadata %>%
  group_by(sample, study_id, sample_supplier_name, sample_cram_file) %>%
  summarise(total_sample_reads = sum(total_reads), .groups = 'keep')
```

```{r show_sample_metadata, echo = FALSE}
# Present to user as a searchable table
datatable(sample_metadata, class = 'white-space: nowrap', filter = 'top', options = list(scrollX = TRUE, autoWidth = FALSE, pageLength = 10), rownames = FALSE)
```

## Guide library

The Yusa v1.1 library (101090 guides, 18025 genes, 1004 non-targeting controls) has been reformatted for use with  [pyCROQUET](https://github.com/cancerit/pycroquet) - the specific library format is described [here](https://github.com/cancerit/pycroquet/wiki/Guide-library-format).  

```{r read_and_format_library}
# Read in the original library
original_library <- read.delim(file.path(repo_path, 'METADATA', 'library_CRISPRcleanR_KY_Library_v1.1.tsv'), header = T, sep = "\t")

# Parse for pyCROQUET
pyc_library <- original_library %>%
  select('sgrna_ids' = CODE, 'sgrna_seqs' = seq, 'gene_pair_id' = GENES) %>%
  arrange(gene_pair_id, sgrna_ids) %>%
  mutate(`#id` = row_number(), .before = 'sgrna_ids')

# Set the pyCROQUET library file path
pyc_library_path <- file.path(repo_path, 'METADATA', 'library_CRISPRcleanR_KY_Library_v1.1.pycroquet.tsv') 

# Open a connection
fileConn <- file(pyc_library_path)

# Write header lines to the pyCROQUET library file
write_lines("##library-type: single", pyc_library_path, append = F)
write_lines("##library-name: Yusa v1.1", pyc_library_path, append = T)
write_lines("##species: human", pyc_library_path, append = T)

# Write library to the pyCROQUET library file 
suppressWarnings(write.table(pyc_library, file = pyc_library_path, sep = "\t", append = TRUE , row.names = FALSE, quote = FALSE))

# Close the connection
close(fileConn)
```

Here we show the first 10 lines of the pyCROQUET-formatted library:

```{r show_pyc_library, echo = FALSE}
datatable(head(pyc_library), class = 'white-space: nowrap', filter = 'top', options = list(scrollX = TRUE, autoWidth = FALSE), rownames = FALSE)
```

## Quantification

[pyCROQUET](https://github.com/cancerit/pycroquet) is a command-line tool for the quantification of single and combinatorial CRISPR screens. As inputs it requires: the guide library (formatted according to the [wiki](https://github.com/cancerit/pycroquet/wiki/Guide-library-format) specifications) and sequencing data. Here, we are using CRAM files, one per sample, as our sequencing data input. 

pyCROQUET version 1.5.1 was used for quantification on the Sanger compute farm (farm5) which uses LSF for job submission. Instead of building one job script (i.e. a script which defines both the LSF requirements and the commands you want to run) per sample, we can use a job array (one script which runs the same commands on multiple files in parallel). The jobscript for running pyCROQUET can be found in [SCRIPTS/pycroquet_job_array.sh](SCRIPTS/pycroquet_job_array.sh).

```{r pyc_jobscript}
# Set directory for jobscripts and logs
jobscript_path <- file.path(repo_path, 'SCRIPTS', 'pycroquet_lsf_job_array.sh')
log_directory <- file.path(repo_path, 'LOGS', 'pycroquet')

# Open a connection
fileConn <- file(jobscript_path)

# Set the LSF queue
write_lines('#BSUB -q normal', jobscript_path, append = F)

# Set the LSF job name
write_lines(paste0('#BSUB -J pycroquet[1-', nrow(sample_metadata), ']'), jobscript_path, append = T)

# Set the LSF log file paths
write_lines(paste0('#BSUB -oo "', log_directory, '/pycroquet_single_guide.%I.o"'), jobscript_path, append = T)
write_lines(paste0('#BSUB -eo "', log_directory, '/pycroquet_single_guide.%I.e"'), jobscript_path, append = T)

# Set the LSF memory requirements
write_lines('#BSUB -R "select[mem>10000] rusage[mem=10000] span[hosts=1]"', jobscript_path, append = T)
write_lines('#BSUB -M 10000', jobscript_path, append = T)

# Set the CPU requirements
write_lines('#BSUB -n 8', jobscript_path, append = T)

# Load pyCROQUET
write_lines('module load pycroquet/1.5.1', jobscript_path, append = T)

# Set guide library
write_lines(paste0('guides=\"', pyc_library_path, '\"'), jobscript_path, append = T)

# Set directory where CRAM files are stored
write_lines(paste0('cram_directory=\"', file.path(repo_path, 'DATA', 'CRAM', 'study_sample'), '\"'), jobscript_path, append = T)

# Get list of CRAM files (one per sample) for processing
write_lines('cram_files=($(ls ${cram_directory}/*.cram))', jobscript_path, append = T)

# Set index of array (i.e. which file to use for each job)
write_lines('array_index=$(expr ${LSB_JOBINDEX} - 1)', jobscript_path, append = T)

# Get CRAM file name (i.e. without path) and build output file label (e.g. the sample name)
write_lines('query_file_name=$(basename ${cram_files[${array_index}]})', jobscript_path, append = T)
write_lines('query_file_label=${query_file_name::-5}', jobscript_path, append = T)

# Set the path where pyCROQUET results will be written
write_lines(paste0('output_directory=\"', file.path(repo_path, 'DATA', 'pycroquet'), '\"'), jobscript_path, append = T)

# Build pycroquet command
pyc_cmd <- 'pycroquet single-guide -g ${guides} -q ${cram_directory}/${query_file_name} -o ${output_directory}/${query_file_label} --cpus 8 --boundary-mode exact --chunks 50000'
write_lines(pyc_cmd, jobscript_path, append = T)

# Close connection
close(fileConn)
```

To submit the jobs to LSF:

```
bsub < SCRIPTS/pycroquet_lsf_job_array.sh
```

Note: if this has been run before, you will need to clean out `LOGS/pycroquet` and `DATA/pycroquet`.


## QC

Sample and library statistics were generated by pyCROQUET during quantification. JSON files (one per sample) are read in and combined into a data frame, changing the column names to make them user friendly.

```{r read_in_pyc_results, echo = FALSE}
# Set empty count and statistics matrices
raw_counts <- data.frame()
sample_stats <- data.frame()

# Loop over the samples
for (i in 1:nrow(sample_metadata)) {
  # Build file path
  fn <- paste0(sample_metadata$study_id[i], '_', sample_metadata$sample[i], '.counts.tsv.gz')
  fp <- file.path(repo_path, 'DATA', 'pycroquet', fn)
  
  # Read in sample guide counts
  tmp_counts <- data.frame()
  tmp_counts <- read.delim(file = fp, skip = 2, header = T, sep = "\t",
                           col.names = c('id', 'sgrna_ids', 'sgrna_seqs', 'gene_symbol', 'is_unique_guide', sample_metadata$sample_supplier_name[i]),
                           colClasses = c(rep('character', 4), 'factor', 'numeric'))
  
  # Gather counts by sample
  tmp_counts <- tmp_counts %>%
    gather(sample, counts, -id, -sgrna_ids, -sgrna_seqs, -gene_symbol, -is_unique_guide)
  
  # Add sample counts to main data frame
  if (nrow(raw_counts) == 0) {
    raw_counts <- tmp_counts
  } else {
    raw_counts <- rbind(raw_counts, tmp_counts)
  }
  
  # Set statistics file path
  sfn <- paste0(sample_metadata$study_id[i], '_', sample_metadata$sample[i], '.stats.json')
  sfp <- file.path(repo_path, 'DATA', 'pycroquet', sfn)
  
  # Read in sample statistics 
  tmp_stats <- fromJSON(sfp, flatten = F)
  tmp_stats$sample <- sample_metadata$sample_supplier_name[i]
 
  # Add sample stats to main data frame
  if (i == 1) {
    sample_stats <- sample_stats %>% bind_rows(tmp_stats)
  } else {
    sample_stats <- sample_stats %>% bind_rows(tmp_stats)
  }
}

# Format sample statistics
sample_stats <- sample_stats %>%
  mutate_at(vars(starts_with('low'), ends_with('reads'), contains('guide'), starts_with('total')), as.numeric)
```

### Read-to-guide mapping

Barplot below shows the number of reads (millions) per sample, coloured by the fraction of reads which can be assigned to a guide (mapped = blue) and those which cannot (unmapped = red).

```{r prepare_mapping_stats, echo = FALSE}
# Prepare mapping statistics
mapping_stats <- as_tibble(sample_stats) %>% 
  dplyr::select(sample, 'total' = total_reads, 'mapped' = mapped_to_guide_reads, 'unmapped' = unmapped_reads) %>%
  mutate('mapped_pct' = (as.numeric(mapped) / total) * 100, 'unmapped_pct' = (as.numeric(unmapped) / total) * 100) %>%
  pivot_longer(cols = c('mapped', 'unmapped'), names_to = 'category', values_to = 'num_reads') %>% 
  pivot_longer(cols = c('mapped_pct', 'unmapped_pct'), values_to = 'pct') %>%
  filter((category == 'mapped' & name == 'mapped_pct') | (category == 'unmapped' & name == 'unmapped_pct')) %>%
  mutate('pct_label' = paste0(sprintf("%.0f", as.numeric(pct)), "%"))
mapping_stats$category <- factor(mapping_stats$category, levels = c('unmapped', 'mapped'))

# Plot mapping statisics
mapping_stats_barplot <- 
  ggplot(mapping_stats, aes(x = sample, y = num_reads, fill = category)) +
    geom_bar(position = "stack", stat = "identity") +
    #facet_grid( . ~ cell_line, scales = 'free_x', space = 'free') +
    scale_y_continuous(name = "Number of reads (millions)", breaks = pretty_breaks(), labels = unit_format(unit = "M", scale = 1e-6)) +
    geom_text(aes(label = pct_label), size = 3, hjust = 0.5, vjust = 1.5, position = "stack")  +
    xlab('') +
    scale_fill_nejm(alpha = 0.5) +
    theme_pubr() +
    theme(axis.text.x = element_text(angle = 90, hjust = 1)) 

# Save mapping statistics barplot
ggsave(plot = mapping_stats_barplot, filename = file.path(repo_path, 'DATA', 'QC', 'read_to_guide_mapping_barplot.png'), dpi = 300, device = 'png', width = 12, height = 8)
```

```{r, echo = FALSE, out.width = "80%", out.height = "80%"}
knitr::include_graphics(file.path(repo_path, 'DATA', 'QC', 'read_to_guide_mapping_barplot.png'))
```

***

### Proportion of zero and low guides per sample
  
Based on the __raw__ counts, of the 101090 guides in the library, the proportion of guides per sample with no reads assigned (zero guides = red) and the number of guides with less than 30 reads assigned (low guides = blue) within that sample. 

```{r barplot_zero_count_guides, echo = FALSE}
# Prepare low and zero count guide statistics
low_count_stats <- as_tibble(sample_stats) %>% 
  dplyr::select(sample, 'total' = total_guides, 'low' = low_count_guides_lt_15, 'zero' = zero_count_guides) %>%
  mutate('low_pct' = (as.numeric(low) / total) * 100, 'zero_pct' = (as.numeric(zero) / total) * 100) %>%
  pivot_longer(cols = c('low', 'zero'), names_to = 'category', values_to = 'num_guides') %>% 
  pivot_longer(cols = c('low_pct', 'zero_pct'), values_to = 'pct') %>%
  filter((category == 'low' & name == 'low_pct') | (category == 'zero' & name == 'zero_pct')) %>%
  mutate('pct_label' = paste0(sprintf("%.2f", as.numeric(pct)), "%"))
low_count_stats$category <- factor(low_count_stats$category, levels = c('low', 'zero'))

# Plot low guide statistics
low_count_guide_barplot <- 
  ggplot(low_count_stats, aes(x = sample, y = pct, fill = category)) +
  geom_bar(stat = "identity", position = "identity", alpha = 0.4) +
  scale_y_continuous(name = "Percentage of guides", breaks = pretty_breaks(), limits = c(0, 5)) +
  xlab('') +
  scale_fill_nejm(alpha = 0.5) +
  theme_pubr() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) 

# Save mapping statistics barplot
ggsave(plot = low_count_guide_barplot, filename = file.path(repo_path, 'DATA', 'QC', 'low_count_guides_barplot.png'), dpi = 300, device = 'png', width = 12, height = 8)
```

```{r, echo = FALSE, out.width = "80%", out.height = "80%"}
knitr::include_graphics(file.path(repo_path, 'DATA', 'QC', 'low_count_guides_barplot.png'))
```

## Sample comparison

### Plasmid

The sequencing run of the pooled plasmids was previously quantified (C6596666) using fuzzy matching - this allows for sequencing (and other) errors. As pyCROQUET performs exact string matching (discards reads with errors), both sets of plasmid counts are compared below.

```{r compare_plasmid, message = FALSE, echo = FALSE}
# Read in pyCROQUET plasmid counts
pyc_plasmid <- read.delim(file.path(repo_path, 'DATA', 'pycroquet', '3081_3081STDY6354070.counts.tsv.gz'), skip = 2, header = T, sep = "\t")

# Read in existing plasmid counts
C6596666_plasmid <- read.delim(file.path(repo_path, 'METADATA', 'C6596666_yusa1.1.tsv'), sep = "\t", header = T)

# Combine counts
plasmid_counts <- pyc_plasmid %>% 
  select(sgrna_ids, 'pyCROQUET' = reads_ERS1072300) %>%
  left_join(C6596666_plasmid %>% select('sgrna_ids' = sgRNA, 'C6596666' = CRISPR_C6596666.sample), by = 'sgrna_ids') %>%
  mutate('diff' = abs(pyCROQUET - C6596666))

# Plot plasmid count comparison
plasmid_cor <- round(cor(plasmid_counts$pyCROQUET, plasmid_counts$C6596666), 2)

plasmid_correlation <- 
  ggscatter(plasmid_counts, x = "pyCROQUET", y = "C6596666",
    alpha = 0.6,
     color = "diff", # Points color, shape and size
     add = "reg.line",  # Add regression line
     add.params = list(color = "tomato2", linetype = "dashed", size = 0.5), # Customize reg. line
     cor.coef = TRUE, # Add correlation coefficient. see ?stat_cor
     cor.coeff.args = list(method = "pearson", label.x = 3, label.sep = "\n")
     ) + 
    gradient_color("Greys") +
    theme(legend.position = "none") +
    labs(title = "Comparing methods of plasmid quantification",
         caption = str_wrap("Pearson correlation of raw counts using exact matching (pyCROQUET) with pyCROQUET version 1.5.1 or allowing mismatches (C6596666).", 80)) 

# Save mapping statistics barplot
ggsave(plot = plasmid_correlation, filename = file.path(repo_path, 'DATA', 'QC', 'plasmid_correlation.png'), dpi = 300, device = 'png', width = 8, height = 8)
```

```{r, echo = FALSE, out.width = "80%", out.height = "80%"}
knitr::include_graphics(file.path(repo_path, 'DATA', 'QC', 'plasmid_correlation.png'))
```

There is strong correlation between the existing and pyCROQUET counts for the same plasmid sample (Pearson correlation coefficient = 0.96). The greatest outlier is sgPOLR2K_1 which has read counts of 2787 and 9975 from pyCROQUET and C6596666 quantification respectively. As pyCROQUET uses exact matching, it's likely that these counts will be less than those for C6596666 whose quantification method allowed for multiple mismatches or sequencing errors. This was confirmed by manual `grep` or `agrep` of the strongest outliers.

### Mel285

In the first round of screening, one of the Mel285 replicates failed, leaving only 2 replicates (Mel285_c909R2 and Mel285_c909R3) which were sequenced on runs 29289, 29290 and 29293. Mel285 was subsequently re-screened with 3 replicates (Mel285_R5, Mel285_R6, Mel285_R7) which were sequenced on run 29667. In the analysis only the latter 3 replicates will be taken forward, but while we have the data, it can't hurt to compare the two sets of replicates.

```{r mel285_correlation, echo = FALSE}
# Open connection
png(filename = file.path(repo_path, 'DATA', 'QC', 'Mel285_correlation.png'), height = 1500, width = 1500)

# Subset Mel285 raw counts
mel285 <-  raw_counts %>% filter(grepl('Mel285', sample)) %>% spread(sample, counts)

# Build the plot
correlation_plot <- 
  ggpairs(mel285, columns = 6:10, upper = list(continuous = wrap("cor", size = 6)), columnLabels = colnames(mel285)[6:10]) +
  theme_pubr() +
  theme(text = element_text(size = 20), strip.background = element_rect(fill = "white"))

# Print
print(correlation_plot)

# Close connection
invisible(dev.off())
```

```{r, echo = FALSE, out.width = "80%", out.height = "80%"}
knitr::include_graphics(file.path(repo_path, 'DATA', 'QC', 'Mel285_correlation.png'))
```

## Raw count matrix

Preparation of raw count matrix by excluding two Mel285 replicates ()Mel285_c909R2 and Mel285_c909R3 and renaming HumanV1_1_rep1 to Plasmid for readability. 

```{r prepare_raw_count_matrix}
# Prepare raw count matrix
raw_count_matrix <- raw_counts

# Exclude Mel285_c909R2 and Mel285_c909R3
raw_count_matrix <- raw_count_matrix %>% filter(! sample %in% c('Mel285_c909R2', 'Mel285_c909R3'))

# Rename HumanV1_1_rep1 to Plasmid
raw_count_matrix <- raw_count_matrix %>% mutate(sample = ifelse(sample == 'HumanV1_1_rep1', 'Plasmid', sample))

# Spread matrix so samples are columns
raw_count_matrix <- raw_count_matrix %>% spread(sample, counts)

# Remove prefixing Xs in column names starting with digits
colnames(raw_count_matrix) = gsub("^X", "", colnames(raw_count_matrix))

# Write raw count matrix to file
raw_count_matrix_file <- file.path(repo_path, 'DATA', '01_raw_count_matrix.tsv')
write.table(raw_count_matrix, raw_count_matrix_file, sep = "\t", quote = F, row.names = F)
```
